{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sHAkjS2g_OX3nCnVGFdBR3VRQU46SJvO",
      "authorship_tag": "ABX9TyPYw9Xl60389935D3TFc+lO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adsgfsbfx/Speech-emotion-recognition-/blob/main/Emotion_Recognition_from_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
        "import librosa\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to play the audio files\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM,BatchNormalization , GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "KCdjMVG1vSYI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAVD=\"/content/audio_speech_actors_01-24\""
      ],
      "metadata": {
        "id": "moQuBe1VvWkK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirl_list = os.listdir(RAVD)\n",
        "dirl_list.sort()\n",
        "\n",
        "emotion = []\n",
        "gender = []\n",
        "path = []\n",
        "for i in dirl_list:\n",
        "    fname = os. listdir(os.path. join (RAVD, i))\n",
        "    for f in fname:\n",
        "        part = f.split('.')[0].split('-')\n",
        "        emotion.append(int(part[2]))\n",
        "        temp = int(part[6])\n",
        "        if temp%2 == 0:\n",
        "            temp = \"female\"\n",
        "        else:\n",
        "            temp = \"male\"\n",
        "        gender.append(temp)\n",
        "        path.append(RAVD + i + '/' + f)\n",
        "\n",
        "\n",
        "RAVD_df = pd.DataFrame(emotion)\n",
        "RAVD_df = RAVD_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
        "RAVD_df = pd.concat([pd.DataFrame(gender),RAVD_df],axis=1)\n",
        "RAVD_df.columns = ['gender','emotion']\n",
        "RAVD_df['labels'] =RAVD_df.gender + '_' + RAVD_df.emotion\n",
        "RAVD_df['source'] = 'RAVDESS'\n",
        "RAVD_df = pd.concat([RAVD_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
        "RAVD_df = RAVD_df.drop(['gender', 'emotion'], axis=1)\n",
        "RAVD_df.labels.value_counts()\n"
      ],
      "metadata": {
        "id": "Hf59QSISvc7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.title('Count of Emotions', size=16)\n",
        "sns.countplot(RAVD_df.labels)\n",
        "plt.ylabel('Count', size=12)\n",
        "plt.xlabel('Emotions', size=12)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7lFu9CPmvkN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fRA1= RAVD + '/Actor_08/03-01-03-02-02-01-08.wav'\n",
        "data, sr = librosa.load(fRA1)\n",
        "ipd.Audio(fRA1)"
      ],
      "metadata": {
        "id": "rZxeOs3gvmrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000)\n",
        "spectrogram = librosa.power_to_db(spectrogram)\n",
        "librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\n",
        "plt.title('Mel Spectrogram - Female Happy')\n",
        "plt.colorbar(format='%+2.0f dB')"
      ],
      "metadata": {
        "id": "JtEeO9Rsvp__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fRA2=RAVD +'/Actor_08/03-01-06-01-01-01-08.wav'\n",
        "data, sr = librosa.load(fRA2)\n",
        "ipd.Audio(fRA2)"
      ],
      "metadata": {
        "id": "A-LrOOs-vszL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000)\n",
        "spectrogram = librosa.power_to_db(spectrogram)\n",
        "librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\n",
        "plt.title('Mel Spectrogram - Female Fear')\n",
        "plt.colorbar(format='%+2.0f dB');"
      ],
      "metadata": {
        "id": "4lpXV6djvwW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fRA1 =RAVD +'/Actor_20/03-01-08-02-02-02-20.wav'\n",
        "data, sr = librosa.load(fRA1)\n",
        "ipd.Audio(fRA1)"
      ],
      "metadata": {
        "id": "5bUkmI0xvyc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000)\n",
        "spectrogram = librosa.power_to_db(spectrogram)\n",
        "librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\n",
        "plt.title('Mel Spectrogram - Female Disgust')\n",
        "plt.colorbar(format='%+2.0f dB');"
      ],
      "metadata": {
        "id": "fIOpbUeJv7Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fRA1 = RAVD + '/Actor_19/03-01-04-01-02-01-19.wav'\n",
        "data, sr = librosa.load(fRA1)\n",
        "ipd.Audio(fRA1)"
      ],
      "metadata": {
        "id": "uZivhcbjv4rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000)\n",
        "spectrogram = librosa.power_to_db(spectrogram)\n",
        "librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\n",
        "plt.title('Mel Spectrogram - Male Fearfull')\n",
        "plt.colorbar(format='%+2.0f dB');\n"
      ],
      "metadata": {
        "id": "5LSX-7dywBmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install resampy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYSQz_cpv3-x",
        "outputId": "16c8fe23-dd3b-4be4-e8df-fe1c89f7e335"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: resampy in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.23.5)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.41.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/Actor_18/03-01-05-01-01-01-18.wav\"\n",
        "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "\n",
        "\n",
        "# MFCC\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.subplot(3,1,1)\n",
        "librosa.display.specshow(mfcc, x_axis='time')\n",
        "plt.ylabel('MFCC')\n",
        "plt.colorbar()\n",
        "\n",
        "ipd.Audio(path)"
      ],
      "metadata": {
        "id": "cucNc1U5wF-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/Actor_17/03-01-05-01-01-01-17.wav\"\n",
        "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "\n",
        "\n",
        "# MFCC\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.subplot(3,1,1)\n",
        "librosa.display.specshow(mfcc, x_axis='time')\n",
        "plt.ylabel('MFCC')\n",
        "plt.colorbar()\n",
        "\n",
        "ipd.Audio(path)"
      ],
      "metadata": {
        "id": "U55XxJV1wHUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/audio_speech_actors_01-24/Actor_18/03-01-05-01-01-01-18.wav\"\n",
        "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "female = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "female = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
        "print(len(female))\n",
        "\n",
        "# Gender - Male; Emotion - angry\n",
        "path = \"/content/audio_speech_actors_01-24/Actor_17/03-01-05-01-01-02-17.wav\"\n",
        "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "male = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "male = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
        "print(len(male))\n",
        "\n",
        "# Plot the two audio waves together\n",
        "plt.figure(figsize=(16,10))\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(female, label='female')\n",
        "plt.plot(male, label='male')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "NJMl8hTbwJc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/audio_speech_actors_01-24/Actor_20/03-01-08-02-01-02-20.wav\"\n",
        "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "female = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "female = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
        "print(len(female))\n",
        "\n",
        "# Gender - Male; Emotion - Surprised\n",
        "path = \"/content/audio_speech_actors_01-24/Actor_21/03-01-08-02-01-01-21.wav\"\n",
        "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "male = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
        "male = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
        "print(len(male))\n",
        "\n",
        "# Plot the two audio waves together\n",
        "plt.figure(figsize=(16,10))\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(female, label='female')\n",
        "plt.plot(male, label='male')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "jPXGA_vQwLvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOISE\n",
        "def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "# STRETCH\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate)\n",
        "# SHIFT\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "# PITCH\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)"
      ],
      "metadata": {
        "id": "RDAenwod1w8U"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = np.array(RAVD_df ['path']) [471]\n",
        "if os.path.exists(path):\n",
        "  data, sample_rate = librosa. load (path)"
      ],
      "metadata": {
        "id": "KDoA3owZ12hT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa.display\n",
        "plt. figure(figsize=(12, 5))\n",
        "librosa.display.waveshow(y=data, sr=sample_rate)\n",
        "Audio (data=data, rate=sample_rate)"
      ],
      "metadata": {
        "id": "sF5D9Nic2Bth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = noise(data)\n",
        "plt.figure(figsize=(12,5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "Ko-IPvzm2F7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = (data)\n",
        "plt.figure(figsize=(12, 5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "Tu3GiIvb2J4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = shift(data)\n",
        "plt.figure(figsize=(12,5))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "Uw-R1h1-2O2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os#\n",
        "for dirname, _, filenames in os.walk(''):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "5VVj11laxf3U"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv = '/content/drive/MyDrive/features.csv'\n",
        "Features = pd.read_csv(csv)"
      ],
      "metadata": {
        "id": "ZWenksJbx8KA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features.head()"
      ],
      "metadata": {
        "id": "oNmrQnEiyVZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = Features.iloc[: ,:-1].values\n",
        "Y = Features['labels'].values\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=24, shuffle=True)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmUQ2uN7yZDy",
        "outputId": "1887e047-d5be-47cf-d8c2-7af5ca1adc10"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3240, 20, 1), (3240, 16), (1080, 20, 1), (1080, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(2048, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "model.add(LSTM(128))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(16, activation='softmax'))\n",
        "\n",
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "C-8YnF1Vy9N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=200, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "R8I3G6W4zJ-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
      ],
      "metadata": {
        "id": "7pZ0gNHU0jFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}